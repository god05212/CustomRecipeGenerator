import streamlit as st
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

st.set_page_config(page_title="ë§ì¶¤í˜• ë ˆì‹œí”¼ ìƒì„±ê¸°", page_icon="ğŸ³")
st.title("ğŸ³ ë§ì¶¤í˜• ë ˆì‹œí”¼ ìƒì„±ê¸°")
st.write("ì…ë ¥í•œ ì¬ë£Œë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ìš”ë¦¬ì™€ ë‹¨ê³„ë³„ ë ˆì‹œí”¼ë¥¼ ìƒì„±í•´ë“œë¦½ë‹ˆë‹¤.")

@st.cache_resource(show_spinner=False)
def load_model():
    model_name = "skt/kogpt2-base-v2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    return pipeline("text-generation", model=model, tokenizer=tokenizer)

text_generator = load_model()

ingredients = st.text_area(
    "ğŸ“ ì‚¬ìš©í•˜ê³  ì‹¶ì€ ì¬ë£Œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„)",
    placeholder="ì˜ˆ: ê³„ë€, ìš°ìœ , ë°€ê°€ë£¨"
)

def clean_output(text):
    lines = text.split('\n')
    cleaned = []
    prev = None
    for line in lines:
        if line == prev:
            break
        cleaned.append(line)
        prev = line
    return '\n'.join(cleaned)

if st.button("ë ˆì‹œí”¼ ìƒì„±í•˜ê¸°") and ingredients.strip():
    with st.spinner("ë ˆì‹œí”¼ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        prompt = f"""
ë‹¤ìŒ ì¬ë£Œë“¤ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ìš”ë¦¬ë¥¼ í•˜ë‚˜ ì¶”ì²œí•´ì£¼ì„¸ìš”.
ìš”ë¦¬ ì´ë¦„, í•„ìš”í•œ ì¬ë£Œ ëª©ë¡, ê·¸ë¦¬ê³  ì¡°ë¦¬ë²•ì„ ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ ìì„¸íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

ì¬ë£Œ: {ingredients}

ë‹µë³€ í˜•ì‹:
ìš”ë¦¬ ì´ë¦„: <ìš”ë¦¬ ì´ë¦„>
ì¬ë£Œ:
- ì¬ë£Œ1
- ì¬ë£Œ2
...
ì¡°ë¦¬ë²•:
1. ì²« ë²ˆì§¸ ë‹¨ê³„
2. ë‘ ë²ˆì§¸ ë‹¨ê³„
...
"""

        try:
            output = text_generator(prompt, max_new_tokens=200, temperature=0.7)[0]["generated_text"]
            result = output[len(prompt):].strip()
            clean_result = clean_output(result)
            st.success("âœ… ë ˆì‹œí”¼ ìƒì„± ì™„ë£Œ!")
            st.markdown(clean_result)
        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ:\n\n{e}")
else:
    st.info("ì¬ë£Œë¥¼ ì…ë ¥í•˜ê³  'ë ˆì‹œí”¼ ìƒì„±í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
