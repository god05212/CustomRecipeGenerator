import streamlit as st
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Streamlit ì•± ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ë§ì¶¤í˜• ë ˆì‹œí”¼ ìƒì„±ê¸°", page_icon="ğŸ³")
st.title("ğŸ³ ë§ì¶¤í˜• ë ˆì‹œí”¼ ìƒì„±ê¸°")
st.write("ì…ë ¥í•œ ì¬ë£Œë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ìš”ë¦¬ì™€ ë‹¨ê³„ë³„ ë ˆì‹œí”¼ë¥¼ ìƒì„±í•´ë“œë¦½ë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¡œì»¬ ëª¨ë¸ ë¡œë”© (ìµœì´ˆ ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ì¢€ ê±¸ë¦½ë‹ˆë‹¤)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def load_model():
    model_name = "tiiuae/falcon-7b-instruct"  # ë˜ëŠ” "gpt2", "mistralai/Mistral-7B-Instruct-v0.1" ë“±
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")
    return pipeline("text-generation", model=model, tokenizer=tokenizer)

text_generator = load_model()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ìš©ì ì…ë ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ingredients = st.text_area(
    "ğŸ“ ì‚¬ìš©í•˜ê³  ì‹¶ì€ ì¬ë£Œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„)",
    placeholder="ì˜ˆ: ê³„ë€, ìš°ìœ , ë°€ê°€ë£¨, ì„¤íƒ•"
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë ˆì‹œí”¼ ìƒì„± ë²„íŠ¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.button("ë ˆì‹œí”¼ ìƒì„±í•˜ê¸°") and ingredients.strip():
    with st.spinner("ë ˆì‹œí”¼ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        prompt = f"""
        ë‹¤ìŒ ì¬ë£Œë“¤ì„ ì‚¬ìš©í•´ì„œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ìš”ë¦¬ í•˜ë‚˜ë¥¼ ì¶”ì²œí•´ì¤˜.
        ê·¸ë¦¬ê³  ê·¸ ìš”ë¦¬ì˜ ì´ë¦„ê³¼, ê° ë‹¨ê³„ê°€ ë¶„ë¦¬ëœ ì¡°ë¦¬ë²•ì„ ìƒì„¸íˆ ì•Œë ¤ì¤˜.
        í˜•ì‹ì€ ì•„ë˜ì™€ ê°™ê²Œ í•´ì¤˜:

        ìš”ë¦¬ ì´ë¦„: <ìš”ë¦¬ëª…>

        ì¬ë£Œ: <ì‚¬ìš© ì¬ë£Œ ëª©ë¡>

        ì¡°ë¦¬ë²•:
        1. ...
        2. ...
        3. ...

        ì¬ë£Œ ëª©ë¡: {ingredients}
        """

        try:
            output = text_generator(prompt, max_new_tokens=300, temperature=0.7)[0]["generated_text"]
            # í”„ë¡¬í”„íŠ¸ ì´í›„ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            result = output[len(prompt):].strip()
            st.success("âœ… ë ˆì‹œí”¼ ìƒì„± ì™„ë£Œ!")
            st.markdown(result)

        except Exception as e:
            st.error(f"âŒ ë ˆì‹œí”¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ:\n\n{e}")
else:
    st.info("ì¬ë£Œë¥¼ ì…ë ¥í•œ í›„, 'ë ˆì‹œí”¼ ìƒì„±í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
