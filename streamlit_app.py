import streamlit as st
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

st.set_page_config(page_title="ë§ì¶¤í˜• ë ˆì‹œí”¼ ìƒì„±ê¸°", page_icon="ğŸ³")
st.title("ğŸ³ ë§ì¶¤í˜• ë ˆì‹œí”¼ ìƒì„±ê¸°")
st.write("ì…ë ¥í•œ ì¬ë£Œë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ìš”ë¦¬ì™€ ë‹¨ê³„ë³„ ë ˆì‹œí”¼ë¥¼ ìƒì„±í•´ë“œë¦½ë‹ˆë‹¤.")

@st.cache_resource
def load_model():
    model_name = "skt/kogpt2-base-v2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    return pipeline("text-generation", model=model, tokenizer=tokenizer)

text_generator = load_model()

ingredients = st.text_area(
    "ğŸ“ ì‚¬ìš©í•˜ê³  ì‹¶ì€ ì¬ë£Œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„)",
    placeholder="ì˜ˆ: ê³„ë€, ìš°ìœ , ë°€ê°€ë£¨"
)

if st.button("ë ˆì‹œí”¼ ìƒì„±í•˜ê¸°") and ingredients.strip():
    with st.spinner("ë ˆì‹œí”¼ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        prompt = f"""
ë‹¤ìŒ ì¬ë£Œë“¤ì„ ì‚¬ìš©í•´ì„œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ìš”ë¦¬ë¥¼ ì¶”ì²œí•´ì¤˜.
ìš”ë¦¬ ì´ë¦„, í•„ìš”í•œ ì¬ë£Œ ëª©ë¡, ê·¸ë¦¬ê³  ì¡°ë¦¬ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ì¤˜.

í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ê²Œ ì‘ì„±í•´ì¤˜:

ìš”ë¦¬ ì´ë¦„: <ìš”ë¦¬ëª…>

ì¬ë£Œ: <ì¬ë£Œ ëª©ë¡>

ì¡°ë¦¬ë²•:
1. ...
2. ...
3. ...

ì¬ë£Œ ëª©ë¡: {ingredients}
"""

        try:
            output = text_generator(prompt, max_new_tokens=300, temperature=0.7)[0]["generated_text"]
            result = output[len(prompt):].strip()
            st.success("âœ… ë ˆì‹œí”¼ ìƒì„± ì™„ë£Œ!")
            st.markdown(result)
        except Exception as e:
            st.error(f"âŒ ë ˆì‹œí”¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ:\n\n{e}")
else:
    st.info("ì¬ë£Œë¥¼ ì…ë ¥í•œ í›„, 'ë ˆì‹œí”¼ ìƒì„±í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
