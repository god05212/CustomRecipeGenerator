import streamlit as st
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

st.set_page_config(page_title="ë§ì¶¤í˜• ë ˆì‹œí”¼ ìƒì„±ê¸°", page_icon="ğŸ³")
st.title("ğŸ³ ë§ì¶¤í˜• ë ˆì‹œí”¼ ìƒì„±ê¸°")
st.write("ì…ë ¥í•œ ì¬ë£Œë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ìš”ë¦¬ì™€ ë‹¨ê³„ë³„ ë ˆì‹œí”¼ë¥¼ ìƒì„±í•´ë“œë¦½ë‹ˆë‹¤.")

@st.cache_resource
def load_model():
    model_name = "skt/kogpt2-base-v2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    return pipeline("text-generation", model=model, tokenizer=tokenizer)

text_generator = load_model()

ingredients = st.text_area(
    "ğŸ“ ì‚¬ìš©í•˜ê³  ì‹¶ì€ ì¬ë£Œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„)",
    placeholder="ì˜ˆ: ê³„ë€, ìš°ìœ , ë°€ê°€ë£¨"
)

if st.button("ë ˆì‹œí”¼ ìƒì„±í•˜ê¸°") and ingredients.strip():
    with st.spinner("ë ˆì‹œí”¼ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        prompt = f"""ë‹¤ìŒ ì¬ë£Œë“¤ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ìš”ë¦¬ë¥¼ ì¶”ì²œí•˜ê³ ,
ìš”ë¦¬ ì´ë¦„, ì¬ë£Œ ëª©ë¡, ê·¸ë¦¬ê³  ë‹¨ê³„ë³„ ì¡°ë¦¬ë²•ì„ ì°¨ë¡€ëŒ€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.

ì¬ë£Œ: {ingredients}
"""

        try:
            output = text_generator(prompt, max_new_tokens=150, temperature=0.6)[0]["generated_text"]
            result = output[len(prompt):].strip()

            # ë°˜ë³µ ì œê±° í•¨ìˆ˜ ì ìš©
            def clean_output(text):
                lines = text.split('\n')
                cleaned = []
                prev = None
                for line in lines:
                    if line == prev:
                        break
                    cleaned.append(line)
                    prev = line
                return '\n'.join(cleaned)

            clean_result = clean_output(result)
            st.success("âœ… ë ˆì‹œí”¼ ìƒì„± ì™„ë£Œ!")
            st.markdown(clean_result)
        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ:\n\n{e}")

else:
    st.info("ì¬ë£Œë¥¼ ì…ë ¥í•˜ê³  'ë ˆì‹œí”¼ ìƒì„±í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
